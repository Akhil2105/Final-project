
import numpy as np
import pandas as pd 
import imageio
import matplotlib.pyplot as plt
%matplotlib inline
from numpy.random import seed
seed(123)
import os
import random
import seaborn as sns
import cv2
from PIL import Image
import torch
from torchvision import transforms, utils
from tqdm.auto import tqdm
import glob as gb 
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
print(os.listdir("../data"))

for dirname, _, filenames in os.walk('archive/'):
    for filename in filenames:
        print(os.path.join(dirname, filename))


image_path = ["archive/"+"data"+i+"/"+"data"+i+"/CameraRGB/" for i in ['A', 'B', 'C', 'D', 'E']]
mask_path = ["archive/"+"data"+i+"/"+"data"+i+"/CameraSeg/" for i in ['A', 'B', 'C', 'D', 'E']]
def display_original_image_vs_mask():
    for i in range(5):
        img_path=image_path[i]
        mk_path=mask_path[i]
        img_name=random.choice(os.listdir(img_path))
        img=cv2.imread(os.path.join(img_path, img_name))
        mask=cv2.imread(os.path.join(mk_path, img_name))
        fig, arr=plt.subplots(1,2)
        arr[0].imshow(img)
        arr[0].set_title('Original Scene Image')
        arr[1].imshow(mask[:,:,2])
        arr[1].set_title('Mask Scene Image')
        print(img.shape)
display_original_image_vs_mask()

dataset_subfolders = ['dataA', 'dataB', 'dataC', 'dataD', 'dataE']
data_dir = "archive"
IMG_SIZE = 512

def plot_specific_subfolder_fxn(dataset , dataset_subfolders) : 
    for sub in dataset_subfolders : 
        imgfolder_path = os.path.join(dataset , sub , sub , 'CameraRGB')
        maskfolder_path = os.path.join(dataset , sub , sub , 'CameraSeg')
        img_files = os.listdir(imgfolder_path) 
        mask_files = os.listdir(maskfolder_path)
        
        for i in range(3) : 
            img_path = os.path.join(imgfolder_path , img_files[i])
            mask_path = os.path.join(maskfolder_path , mask_files[i])
            img = cv2.cvtColor(cv2.imread(img_path) , cv2.COLOR_BGR2RGB)
            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
            plt.figure(figsize=(10, 5))
            plt.subplot(1, 2, 1)
            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            plt.title(f'Image {i+1} - Subfolder: {sub}')
            plt.axis('off')
            plt.subplot(1, 2, 2)
            plt.imshow(mask)
            plt.title(f'Mask {i+1} - Subfolder: {sub}')
            plt.axis('off')
            plt.show()
plot_specific_subfolder_fxn(data_dir , dataset_subfolders)
images_paths = [] 
masks_paths = [] 
for sub in tqdm(dataset_subfolders) : 
    img_files = sorted(gb.glob(os.path.join(str(data_dir + "/" + dataset_subfolders[1] + "/" + dataset_subfolders[1] + "/" + 'CameraRGB') , "*")))
    for file in img_files : 
        images_paths.append(file) 
    mask_files = sorted(gb.glob(os.path.join(str(data_dir + "/" + dataset_subfolders[1] + "/" + dataset_subfolders[1] + "/" + 'CameraSeg') , "*")))
    for file in mask_files : 
        masks_paths.append(file)

len(images_paths) , len(masks_paths)

train_images , test_images , train_masks , test_masks = train_test_split(images_paths , masks_paths , test_size = 0.2)
class DataStandard(Dataset) : 
    def __init__(self, img_path , mask_path) : 
        self.img_path = img_path 
        self.mask_path = mask_path
        self.transform = transforms.Compose([
            transforms.Resize(size = (IMG_SIZE , IMG_SIZE)) , 
            ToTensor()
        ])
        if len(self.img_path) != len(self.mask_path) : 
            raise InvalidDatasetException(self.img_path , self.mask_path)
    def __getitem__(self , idx) : 
        image = Image.open(self.img_path[idx])
        tensor_image = self.transform(image)
        mask = Image.open(self.mask_path[idx])
        tensor_mask = self.transform(mask)
        return tensor_image , tensor_mask
    def __len__(self) : 
        return len(self.img_path)
import torchvision.transforms as transforms
from torchvision.transforms import ToTensor , CenterCrop
train_set = DataStandard(train_images , train_masks)
print(f"Total train set images : {train_set.__len__()}")
filer_train=train_set.__getitem__(1000)
filer_train[0].shape , filer_train[1].shape
test_set = DataStandard(test_images , test_masks)
print(f"Total test set images : {test_set.__len__()}")
filer_test=test_set.__getitem__(10)
filer_test[0].shape , filer_test[1].shape
**Dataloaders**
BATCH_SIZE = 8
torch.manual_seed(42)
train_dataloader = DataLoader(
    dataset = train_set , 
    batch_size = BATCH_SIZE , 
    shuffle = True
)
torch.manual_seed(42)
test_dataloader = DataLoader(
    dataset = test_set , 
    batch_size = 10 , 
    shuffle = False
)
print(f"Dataset Train Dataloader {len(train_dataloader)} batch size {BATCH_SIZE}")
trainimage_sample , trainmask_sample = next(iter(train_dataloader))
trainimage_sample.shape , trainmask_sample.shape
fig , axis = plt.subplots(3 , 2 , figsize = (25,20)) 
for i in range(3): 
    img1 = trainimage_sample[i].numpy()
    img1 = np.transpose(img1, (1,2,0))
    axis[i, 0].imshow(img1)
    axis[i, 0].set(title = f"Original Image")
    img2 = trainmask_sample[i].numpy()
    img2 = np.transpose(img2, (1,2,0))
    axis[i, 1].imshow(img2[:,:,0])
    axis[i, 1].set(title = f"Masked Image")
plt.subplots_adjust(wspace=0.0)
**Model Building**
import torch.nn as nn 

class ConvolutionStructure(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ConvolutionStructure, self).__init__()
        self.block = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),    
            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    def forward(self, x: torch.Tensor):
        return self.block(x)
class networkpush(nn.Module):
    def forward(self, x: torch.Tensor, skip_connection: torch.Tensor):
        _, _, h, w = skip_connection.shape
        crop = CenterCrop((h, w))(x)
        residual = torch.cat((x, crop), dim=1)
        return residual
<br>This connection allows the model to maintain fine-grained details during the upsampling process. Skip connections are a fundamental technique for improving segmentation results, particularly for preserving object boundaries and intricate patterns.
class UNET(nn.Module) : 
    def __init__(self , in_channels, out_channels) : 
        super().__init__() 
        self.encoders = nn.ModuleList([
            ConvolutionStructure(in_channels, 64),
            ConvolutionStructure(64, 128),
            ConvolutionStructure(128, 256),
            ConvolutionStructure(256, 512),
        ])
        
        self.pool = nn.MaxPool2d(2)
        self.networkpush = networkpush()
        self.decoders = nn.ModuleList([
            ConvolutionStructure(1024, 512),
            ConvolutionStructure(512, 256),
            ConvolutionStructure(256, 128),
            ConvolutionStructure(128, 64),
        ])
        self.up_samples = nn.ModuleList([
            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),
            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),
            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),
            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        ])
        self.bottleneck = ConvolutionStructure(512, 1024)
        self.finalconv = nn.Conv2d(64, out_channels, kernel_size=1, stride=1)
    def forward(self , x ) : 
        skip_connections = []
        for enc in self.encoders:
            x = enc(x)
            skip_connections.append(x)
            x = self.pool(x)
        x = self.bottleneck(x)
        for idx, dec in enumerate(self.decoders):
            x = self.up_samples[idx](x)
            skip_connection = skip_connections.pop()
            x = self.networkpush(x, skip_connection)
            x = dec(x)
        x = self.finalconv(x) 
        return x
device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
model=UNET(
    in_channels= 3 , 
    out_channels=3
).to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)
epochs = 40
training_loss = [] 

for i in tqdm(range(epochs)) : 
    epoch_loss = 0 
    for batch , (image , mask) in enumerate(train_dataloader) : 
        image , mask = image.to(device) , mask.to(device)
        mask_pred = model(image) 
        loss = criterion(mask_pred , mask)
        if batch % 500 == 0:
            print(f"Sample scene image recognition {batch * len(image)}/{len(train_dataloader.dataset)} view.") 
        loss.backward()
        optimizer.step()
        optimizer.zero_grad() 
        epoch_loss +=loss.item()
    training_loss.append((epoch_loss/len(train_dataloader)))
    print(f"Epoch  : {i+1} , Loss: {(epoch_loss/len(train_dataloader))}\n\n")
print(f"Training set loss: {training_loss[-1]}")
plt.plot(range(epochs) , training_loss,color="red",label="Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
a loss function is essential for quantifying the difference between the model's predictions and the actual data. In this code, the Binary Cross-Entropy with Logits Loss (BCEWithLogitsLoss) is selected as the loss function.<br>

This is a common choice for binary image segmentation tasks, and it measures the dissimilarity between predicted values and true labels. The Adam optimizer is used to adjust the model's parameters based on the computed gradients, making the model progressively better at its task.
**PerformanceEvaluation**
test_loss = 0
with torch.no_grad() : 
    for image , mask in tqdm(test_dataloader) : 
        image , mask = image.to(device) , mask.to(device)
        mask_pred = model(image)
        loss = criterion(mask_pred , mask) 
        test_loss += loss
test_loss/=len(test_dataloader)
print(f"Testing set loss: {test_loss}\n")
After training, the code proceeds to evaluate the model's performance on a test dataset. The test dataset is different from the training data, and the code calculates the loss on this dataset. 

This test loss is an important metric for assessing how well the model generalizes to unseen data. A lower test loss implies that the model is effective in making accurate predictions on new, unseen images.
def image_prediction_figure_plot(img_sample , mask_sample , val_pred):
    fig , axis = plt.subplots(8 , 3 , figsize = (20,20)) 
    for i in range(8): 
        img1 = img_sample[i].numpy()
        img1 = np.transpose(img1, (1,2,0))
        axis[i, 0].imshow(img1)
        axis[i, 0].set(title = f"Original Scene Image")
        img2 = mask_sample[i].numpy()
        img2 = np.transpose(img2, (1,2,0))
        axis[i, 1].imshow(img2[:,:,0])
        axis[i, 1].set(title = f"Masked Scene Image")
        img3 = val_pred[i].cpu().numpy()
        img3 = np.transpose(img3, (1,2,0))
        axis[i, 2].imshow(img3[:,:,0])
        axis[i, 2].set(title = f"Predicted Mask Image")
    plt.subplots_adjust(wspace=0.5)
with torch.no_grad() : 
        img_sample , mask_sample = next(iter(test_dataloader)) 
        img_sample , mask_sample = img_sample.to(device) , mask_sample.to(device) 
        test_pred = model(img_sample)
        image_prediction_figure_plot(img_sample.cpu() , mask_sample.cpu() , test_pred.detach())
torch.save(model.state_dict(), 'SelfDrivingCarScenePredictionModel.pt')
a sample of images and their corresponding masks and displays the original scene image, the masked image, and the model's predicted mask image.

This visualization aids in understanding the model's performance qualitatively.after training and evaluating the model, the code saves its learned parameters to a file (in this case, 'SelfDrivingCarScenePredictionModel.pt'). This allows the model to be reused later for making predictions without retraining.
